# Aplicacion Ollama Local Privada con Streamlit

Requisitos:
Instalar [Ollama](https://ollama.com/) 

Luego con pip instalar [Streamlit](https://streamlit.io).

## Características

- **Interfaz Interactiva**: Utiliza Streamlit para crear una interfaz fácil de usar.
- **Ejecución de Modelos Locales**: Ejecuta tus modelos Ollama localmente sin necesidad de APIs externas.
- **Respuestas en Tiempo Real**: Obtén respuestas en tiempo real de tus modelos directamente en la interfaz.


## Instalación
```bash
git https://github.com/ronaldmego/Streamlit_Local_Chat_Ollama.git
```

```bash
pip install -r requirements.txt
```

## Ejecutar el streamlit app
```bash
streamlit Streamlit_Chat_Ollama.py
```

## Creditos
Autor: https://twitter.com/tonykipkemboi

Video tutorial: https://www.youtube.com/watch?v=bAI_jWsLhFM
